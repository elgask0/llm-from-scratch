{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos librerias\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "\n",
    "block_size = 8\n",
    "\n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.1.2\n",
      "Is MPS (Metal Performance Shader) built? True\n",
      "Is MPS available? True\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Comprobamos verison de pytorch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Comprobamos si tenemos MPS arquitectura del chip m2 de apple\n",
    "print(f\"Is MPS (Metal Performance Shader) built? {torch.backends.mps.is_built()}\")\n",
    "print(f\"Is MPS available? {torch.backends.mps.is_available()}\")\n",
    "\n",
    "# Utilizamos mps mas rapido que la cpu\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "device = torch.device(device)\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abrimos el txt como string\n",
    "with open('wizard_of_oz.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Wonderful Wizard of Oz\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chapter I\n",
      "The Cyclone\n",
      "\n",
      "\n",
      "Dorothy lived in the midst of the great Kansas prairies, with Uncle\n",
      "Henry, who was a farmer, and Aunt Em, who was the farmer’s wife. Their\n",
      "house was small, for the lumber to build it had to be carried by wagon\n",
      "many miles. There were four walls, a floor and a roof, which made one\n",
      "room; and this room contained a rusty looking cookstove, a cupboard for\n",
      "the dishes, a table, three or four chairs, and the beds. Uncle Henry\n",
      "and Aunt Em had a big bed in one corner, and Dorothy a little bed in\n",
      "another corner. There was no garret at all, and no cellar—except a\n",
      "small hole dug in the ground, called a cyclone cellar, where the family\n",
      "could go in case one of those great whirlwinds arose, mighty enough to\n",
      "crush any building in its path. It was reached by a trap door in the\n",
      "middle of the floor, from which a ladder led down into the small, dark\n",
      "hole.\n",
      "\n",
      "When Dorothy stood in the doorway and looked around, she could see\n",
      "nothing but the great gray prairie on every side. Not a tree nor a\n",
      "house broke the broad sweep of flat country that reached to the edge of\n",
      "the sky in all directions. The sun had baked the plowed land into a\n",
      "gray mass, with little cracks running through it. Even the grass was\n",
      "not green, for the sun had burned the tops of the long blades until\n",
      "they were the same gray color to be seen everywhere. Once the house had\n",
      "been painted, but the sun blistered the paint and the rains washed it\n",
      "away, and now the house was as dull and gray as everything else.\n",
      "\n",
      "When Aunt Em came there to live she was a young, pretty wife. The sun\n",
      "and wind had changed her, too. They had taken the sparkle from her eyes\n",
      "and left them a sober gray; they had taken the red from her cheeks and\n",
      "lips, and they were gray also. She was thin and gaunt, and never smiled\n",
      "now. When Dorothy, who was an orphan, first came to her, Aunt Em had\n",
      "been so startled by the child’s laughter that she would scream and\n",
      "press her hand upon her heart whenever Dorothy’s merry voice reached\n",
      "her ears; and she still looked at the little girl with wonder that she\n",
      "could find anything to laugh at.\n",
      "\n",
      "Uncle Henry never laughed. He worked hard from morning till night and\n",
      "did not know what joy was. He was gray also, from his long beard to his\n",
      "rough boots, and he looked stern and solemn, and rarely spoke.\n",
      "\n",
      "It was Toto that made Dorothy laugh, and saved her from growing as gray\n",
      "as her other surroundings. Toto was not gray; he was a little black\n",
      "dog, with long silky hair and small black eyes that twinkled merrily on\n",
      "either side of his funny, wee nose. Toto played all day long, and\n",
      "Dorothy played with him, and loved him dearly.\n",
      "\n",
      "Today, however, they were not playing. Uncle Henry sat upon the\n",
      "doorstep and looked anxiously at the sky, which was even grayer than\n",
      "usual. Dorothy stood in the door with Toto in her arms, and looked at\n",
      "the sky too. Aunt Em was washing the dishes.\n",
      "\n",
      "From the far north they heard a low wail of the wind, and Uncle Henry\n",
      "and Dorothy could see where the long grass bowed in waves before the\n",
      "coming storm. There now came a sharp whistling in the air from the\n",
      "south, and as they turned their eyes that way they saw ripples in the\n",
      "grass coming from that direction also.\n",
      "\n",
      "Suddenly Uncle Henry stood up.\n",
      "\n",
      "“There’s a cyclone coming, Em,” he called to his wife. “I’ll go look\n",
      "after the stock.” Then he ran toward the sheds where the cows and\n",
      "horses were kept.\n",
      "\n",
      "Aunt Em dropped her work and came to the door. One glance told her of\n",
      "the danger close at hand.\n",
      "\n",
      "“Quick, Dorothy!” she screamed. “Run for the cellar!”\n",
      "\n",
      "Toto jumped out of Dorothy’s arms and hid under the bed, and the girl\n",
      "started to get him. Aunt Em, badly frightened, threw open the trap door\n",
      "in the floor and climbed down the ladder into the small, dark hole.\n",
      "Dorothy caught Toto at last and started to follow her aunt. When she\n",
      "was halfway across the room there came a great shriek from the wind,\n",
      "and the house shook so hard that she lost her footing and sat down\n",
      "suddenly upon the floor.\n",
      "\n",
      "Then a strange thing happened.\n",
      "\n",
      "The house whirled around two or three times and rose slowly through the\n",
      "air. Dorothy felt as if she were going up in a balloon.\n",
      "\n",
      "The north and south winds met where the house stood, and made it the\n",
      "exact center of the cyclone. In the middle of a cyclone the air is\n",
      "generally still, but the great pressure of the wind on every side of\n",
      "the house raised it up higher and higher, until it was at the very top\n",
      "of the cyclone; and there it remained and was carried miles and miles\n",
      "away as easily as you could carry a feather.\n",
      "\n",
      "It was very dark, and the wind howled horribly around her, but Dorothy\n",
      "found she was riding quite easily. After the first few whirls around,\n",
      "and one other time when the house tipped badly, she felt as if she were\n",
      "being rocked gently, like a baby in a cradle.\n",
      "\n",
      "Toto did not like it. He ran about the room, now here, now there,\n",
      "barking loudly; but Dorothy sat quite still on the floor and waited to\n",
      "see what would happen.\n",
      "\n",
      "Once Toto got too near the open trap door, and fell in; and at first\n",
      "the little girl thought she had lost him. But soon she saw one of his\n",
      "ears sticking up through the hole, for the strong pressure of the air\n",
      "was keeping him up so that he could not fall. She crept to the hole,\n",
      "caught Toto by the ear, and dragged him into the room again, afterward\n",
      "closing the trap door so that no more accidents could happen.\n",
      "\n",
      "Hour after hour passed away, and slowly Dorothy got over her fright;\n",
      "but she felt quite lonely, and the wind shrieked so loudly all about\n",
      "her that she nearly became deaf. At first she had wondered if she would\n",
      "be dashed to pieces when the house fell again; but as the hours passed\n",
      "and nothing terrible happened, she stopped worrying and resolved to\n",
      "wait calmly and see what the future would bring. At last she crawled\n",
      "over the swaying floor to her bed, and lay down upon it; and Toto\n",
      "followed and lay down beside her.\n",
      "\n",
      "In spite of the swaying of the house and the wailing of the wind,\n",
      "Dorothy soon closed her eyes and fell fast asleep.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chapter II\n",
      "The Council with the Munchkins\n",
      "\n",
      "\n",
      "She was awakened by a shock, so sudden and severe that if Dorothy had\n",
      "not been lying on the soft bed she might have been hurt. As it was, the\n",
      "jar made her catch her breath and wonder what had happened; and Toto\n",
      "put his cold little nose into her face and whined dismally. Dorothy sat\n",
      "up and noticed that the house was not moving; nor was it dark, for the\n",
      "bright sunshine came in at the window, flooding the little room. She\n",
      "sprang from her bed and with Toto at her heels ran and opened the door.\n",
      "\n",
      "The little girl gave a cry of amazement and looked about her, her eyes\n",
      "growing bigger and bigger at the wonderful sights she saw.\n",
      "\n",
      "The cyclone had set the house down very gently—for a cyclone—in the\n",
      "midst of a country of marvelous beauty. There were lovely patches of\n",
      "greensward all about, with stately trees bearing rich and luscious\n",
      "fruits. Banks of gorgeous flowers were on every hand, and birds with\n",
      "rare and brilliant plumage sang and fluttered in the trees and bushes.\n",
      "A little way off was a small brook, rushing and sparkling along between\n",
      "green banks, and murmuring in a voice very grateful to a little girl\n",
      "who had lived so long on the dry, gray prairies.\n",
      "\n",
      "While she stood looking eagerly at the strange and beautiful sights,\n",
      "she noticed coming toward her a group of the queerest people she had\n",
      "ever seen. They were not as big as the grown folk she had always been\n",
      "used to; but neither were they very small. In fact, they seemed about\n",
      "as tall as Dorothy, who was a well-grown child for her age, although\n",
      "they were, so far as looks go, many years older.\n",
      "\n",
      "Three were men and one a woman, and all were oddly dressed. They wore\n",
      "round hats that rose to a small point a foot above their heads, with\n",
      "little bells around the brims that tinkled sweetly as they moved. The\n",
      "hats of the men were blue; the little woman’s hat was white, and she\n",
      "wore a white gown that hung in pleats from her shoulders. Over it were\n",
      "sprinkled little stars that glistened in the sun like diamonds. The men\n",
      "were dressed in blue, of the same shade as their hats, and wore\n",
      "well-polished boots with a deep roll of blue at the tops. The men,\n",
      "Dorothy thought, were about as old as Uncle Henry, for two of them had\n",
      "beards. But the little woman was doubtless much older. Her face was\n",
      "covered with wrinkles, her hair was nearly white, and she walked rather\n",
      "stiffly.\n",
      "\n",
      "When these people drew near the house where Dorothy was standing in the\n",
      "doorway, they paused and whispered among themselves, as if afraid to\n",
      "come farther. But the little old woman walked up to Dorothy, made a low\n",
      "bow and said, in a sweet voice:\n",
      "\n",
      "“You are welcome, most noble Sorceress, to the land of the Munchkins.\n",
      "We are so grateful to you for having killed the Wicked Witch of the\n",
      "East, and for setting our people free from bondage.”\n",
      "\n",
      "Dorothy listened to this speech with wonder. What could the little\n",
      "woman possibly mean by calling her a sorceress, and saying she had\n",
      "killed the Wicked Witch of the East? Dorothy was an innocent, harmless\n",
      "little girl, who had been carried by a cyclone many miles from home;\n",
      "and she had never killed anything in all her life.\n",
      "\n",
      "But the little woman evidently expected her to answer; so Dorothy said,\n",
      "with hesitation, “You are very kind, but there must be some mistake. I\n",
      "have not killed anything.”\n",
      "\n",
      "“Your house did, anyway,” replied the little old woman, with a laugh,\n",
      "“and that is the same thing. See!” she continued, pointing to the\n",
      "corner of the house. “There are her two feet, still sticking out from\n",
      "under a block of wood.”\n",
      "\n",
      "Dorothy looked, and gave a little cry of fright. There, indeed, just\n",
      "under the corner of the great beam the house rested on, two feet were\n",
      "sticking out, shod in silver shoes with pointed toes.\n",
      "\n",
      "“Oh, dear! Oh, dear!” cried Dorothy, clasping her hands together in\n",
      "dismay. “The house must have fallen on her. Whatever shall we do?”\n",
      "\n",
      "“There is nothing to be done,” said the little woman calmly.\n",
      "\n",
      "“But who was she?” asked Dorothy.\n",
      "\n",
      "“She was the\n"
     ]
    }
   ],
   "source": [
    "# Inspeccionamos la string\n",
    "print(text[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '(', ')', ',', '-', '.', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '—', '‘', '’', '“', '”', '\\ufeff']\n",
      "69\n"
     ]
    }
   ],
   "source": [
    "# Inspeccionamos todos los caracteres que se utilizan\n",
    "chars = sorted(set(text))\n",
    "vocab_size = len(chars)\n",
    "print(chars)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[44, 41, 48, 48, 51] hello\n"
     ]
    }
   ],
   "source": [
    "# Establecemos el encoder y el decoder\n",
    "\n",
    "# Un encoder en el contexto de LLM (Lenguaje de Modelado de Lenguaje) es la parte del modelo que toma texto de entrada y lo codifica en una representación numérica.\n",
    "# Por ejemplo, en un modelo de traducción, el encoder toma una oración en un idioma y la convierte en una representación que la red neuronal puede entender.\n",
    "\n",
    "# Un decoder, por otro lado, toma la representación numérica generada por el encoder y la decodifica para producir la salida deseada.\n",
    "# En el mismo modelo de traducción, el decoder toma la representación numérica y genera la oración traducida en otro idioma.\n",
    "\n",
    "# En resumen, el encoder convierte texto en números y el decoder convierte esos números de nuevo en texto, lo que permite a las redes neuronales comprender y generar lenguaje humano.\n",
    "\n",
    "string_to_int = { ch:i for i,ch in enumerate(chars) }\n",
    "int_to_string = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [string_to_int[c] for c in s]\n",
    "decode = lambda l: ''.join([int_to_string[i] for i in l])\n",
    "\n",
    "encoded_hello = encode('hello')\n",
    "decoded_hello = decode(encoded_hello)\n",
    "\n",
    "print(encoded_hello, decoded_hello)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([68, 30, 44, 41,  1, 33, 51, 50, 40, 41, 54, 42, 57, 48,  1, 33, 45, 62,\n",
      "        37, 54, 40,  1, 51, 42,  1, 25, 62,  0,  0,  0,  0,  0, 13, 44, 37, 52,\n",
      "        56, 41, 54,  1, 19,  0, 30, 44, 41,  1, 13, 61, 39, 48, 51, 50, 41,  0,\n",
      "         0,  0, 14, 51, 54, 51, 56, 44, 61,  1, 48, 45, 58, 41, 40,  1, 45, 50,\n",
      "         1, 56, 44, 41,  1, 49, 45, 40, 55, 56,  1, 51, 42,  1, 56, 44, 41,  1,\n",
      "        43, 54, 41, 37, 56,  1, 21, 37, 50, 55])\n"
     ]
    }
   ],
   "source": [
    "# En PyTorch, torch.tensor es un objeto que se utiliza para almacenar datos multidimensionales, como matrices o tensores.\n",
    "# Es similar a una lista o un arreglo, pero está diseñado específicamente para realizar operaciones matemáticas eficientes en datos numéricos.\n",
    "\n",
    "# Por ejemplo, puedes crear un tensor para representar una matriz 2x2 de la siguiente manera:\n",
    "# tensor_2x2 = torch.tensor([[1, 2], [3, 4]])\n",
    "\n",
    "# Luego, puedes realizar operaciones matemáticas en ese tensor, como suma, multiplicación, etc.\n",
    "# result = tensor_2x2 + 2  # Suma 2 a cada elemento del tensor\n",
    "\n",
    "# Los tensores son fundamentales en PyTorch y se utilizan ampliamente en tareas de aprendizaje profundo, como el entrenamiento de redes neuronales.\n",
    "\n",
    "data = torch.tensor(encode(text), dtype = torch.long)\n",
    "\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separamos el tensor en datos de entrenamiento y validacion\n",
    "\n",
    "n = int(0.8*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "tensor([[58, 41, 54,  1, 49, 61,  0, 38],\n",
      "        [54, 41, 40,  1, 55, 51, 48, 40],\n",
      "        [48, 48, 61,  5,  1, 66, 19,  1],\n",
      "        [37, 56, 41, 40,  1, 56, 44, 41]], device='mps:0')\n",
      "targets:\n",
      "tensor([[41, 54,  1, 49, 61,  0, 38, 41],\n",
      "        [41, 40,  1, 55, 51, 48, 40, 45],\n",
      "        [48, 61,  5,  1, 66, 19,  1, 40],\n",
      "        [56, 41, 40,  1, 56, 44, 41, 49]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "x, y = get_batch('train')\n",
    "print('inputs:')\n",
    "# print(x.shape)\n",
    "print(x)\n",
    "print('targets:')\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is  tensor([68]) target is  tensor(30)\n",
      "when input is  tensor([68, 30]) target is  tensor(44)\n",
      "when input is  tensor([68, 30, 44]) target is  tensor(41)\n",
      "when input is  tensor([68, 30, 44, 41]) target is  tensor(1)\n",
      "when input is  tensor([68, 30, 44, 41,  1]) target is  tensor(33)\n",
      "when input is  tensor([68, 30, 44, 41,  1, 33]) target is  tensor(51)\n",
      "when input is  tensor([68, 30, 44, 41,  1, 33, 51]) target is  tensor(50)\n",
      "when input is  tensor([68, 30, 44, 41,  1, 33, 51, 50]) target is  tensor(40)\n",
      "El primer block del tensor es tensor([68, 30, 44, 41,  1, 33, 51, 50])\n"
     ]
    }
   ],
   "source": [
    "# Utilizamos la variable block_size definida al principio\n",
    "\n",
    "# En el entrenamiento de Modelos de Lenguaje (LLM), el \"block size\" se utiliza para dividir el texto de entrenamiento en fragmentos más pequeños o bloques.\n",
    "# Cada bloque suele contener una secuencia de palabras o tokens de un tamaño específico.\n",
    "\n",
    "# Por ejemplo, si estamos entrenando un LLM para predecir la siguiente palabra en una oración, podríamos dividir un párrafo largo en bloques de 20 palabras cada uno.\n",
    "# Cada bloque se convierte en una secuencia de entrada para el modelo, y el objetivo del entrenamiento es predecir la siguiente palabra en cada bloque.\n",
    "\n",
    "# El \"block size\" es importante porque afecta la longitud de las secuencias de entrada y, por lo tanto, la capacidad del modelo para capturar dependencias a corto y largo plazo en el texto.\n",
    "\n",
    "# En resumen, en el entrenamiento de LLM, el \"block size\" se utiliza para dividir el texto de entrenamiento en fragmentos manejables que se utilizan como entradas para el modelo durante el entrenamiento.\n",
    "\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size + 1]\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t + 1]\n",
    "    target = y[t]\n",
    "    print('when input is ', context, 'target is ', target)\n",
    "\n",
    "print('El primer block del tensor es', x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "f﻿‘DLuFlV u.j)zcU ku,‘drkcX-hlgijTR?aqQqR-fSvIb﻿mKvhBVDcShqkoUZuA(OIXQ’Go iVGkH!ZM’LKyUNOW(oC)w—TgL’VB.gs U:Aqo—x!gAbw;yRFbgBint?KSQ“WhN“bs.uGQ:GWtqTf—grcU’KIymwk!E Yt?ZIklcR ?k”XmdQcUaxpEAEigN)AdwepGA;KAsxjqT-;vKb )‘jHgeI?zlg UdUaYGW﻿﻿EjNgHqa‘eOWe\n",
      "z-bUb —xKvzBf-VOgwNOfIU)gE?O(C)‘‘ttjfJ\n",
      "R c?lSu,,d—g‘r‘exVKleObU )IH!!,;Ei‘tmweHTEIYbc”Y-fzyww\n",
      "mAgc?K?DYig’;!)‘j“tJ‘yb,,La’V)‘X?)jAf(r‘RAJ‘,u.jJqi—﻿﻿pJvMy CtJva-YjQWlI﻿,! cUZNGWmEiOWyfS—Y;‘XRzFjYV﻿DB‘L;EDcULgyP﻿)‘JAqvSO”oTNWUvk’;Zhk“Lw‘RKSB;w(CDzu UGds\n"
     ]
    }
   ],
   "source": [
    "# Definimos nuestra clase BigramLanguageModel, que es un tipo de red neuronal.\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        # Llamamos al constructor de la clase padre nn.Module.\n",
    "        # Esto es necesario para que PyTorch registre correctamente todas las \n",
    "        # propiedades de nuestra clase como una subclase de nn.Module.\n",
    "        super().__init__()\n",
    "        # Creamos una tabla de embeddings, que es como un diccionario donde cada\n",
    "        # carácter (representado como un índice único) se mapea a un vector de números.\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        \n",
    "    # La función forward define cómo se mueven los datos a través de la red.\n",
    "    def forward(self, index, targets=None):\n",
    "        # Obtenemos los embeddings de los índices de los caracteres proporcionados.\n",
    "        logits = self.token_embedding_table(index)\n",
    "        \n",
    "        # Si no se proporcionan 'targets', no calculamos la pérdida.\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # B es el tamaño del lote (batch size), T es la longitud de la secuencia,\n",
    "            # y C es el tamaño del vocabulario (también la dimensión del embedding aquí).\n",
    "            B, T, C = logits.shape\n",
    "            # Redimensionamos los logits para calcular la pérdida. Los combinamos \n",
    "            # en una sola lista de predicciones para cada carácter en la secuencia.\n",
    "            logits = logits.view(B*T, C)\n",
    "            # Hacemos lo mismo con los 'targets' para que coincidan con la forma de los logits.\n",
    "            targets = targets.view(B*T)\n",
    "            # Calculamos la pérdida de entropía cruzada entre los logits y los targets.\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        # Retornamos tanto los logits como la pérdida.\n",
    "        return logits, loss\n",
    "    \n",
    "    # La función generate crea una secuencia de texto generada por el modelo.\n",
    "    def generate(self, index, max_new_tokens):\n",
    "        # index es un tensor de índices que representa el contexto actual de caracteres.\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Obtenemos las predicciones del modelo llamando a forward.\n",
    "            logits, _ = self.forward(index)\n",
    "            # Nos enfocamos solo en el último paso de tiempo de los logits.\n",
    "            logits = logits[:, -1, :]\n",
    "            # Aplicamos la función softmax para obtener una distribución de probabilidad.\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # Muestreamos un nuevo índice de carácter de esa distribución.\n",
    "            index_next = torch.multinomial(probs, num_samples=1)\n",
    "            # Añadimos el índice del nuevo carácter al final de la secuencia de índices.\n",
    "            index = torch.cat((index, index_next), dim=1)\n",
    "        # Retornamos la secuencia completa de índices generados.\n",
    "        return index\n",
    "\n",
    "\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "m = model.to(device)\n",
    "\n",
    "# context es el tensor que contiene el índice de inicio para la generación de texto.\n",
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "\n",
    "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
    "print(generated_chars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 3e-4\n",
    "eval_iters = 250\n",
    "max_iters = 10000\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Cuadrático Medio (MSE)\n",
    "El MSE es una función de pérdida común utilizada en problemas de regresión, donde el objetivo es predecir una salida continua. Mide la diferencia cuadrada promedio entre los valores predichos y los valores reales, y se utiliza a menudo para entrenar redes neuronales en tareas de regresión.\n",
    "\n",
    "### Descenso del Gradiente (GD)\n",
    "El GD es un algoritmo de optimización utilizado para minimizar la función de pérdida de un modelo de aprendizaje automático. La función de pérdida mide qué tan bien el modelo puede predecir la variable objetivo basándose en las características de entrada. La idea del GD es ajustar iterativamente los parámetros del modelo en la dirección del descenso más pronunciado de la función de pérdida.\n",
    "\n",
    "### Momentum\n",
    "Momentum es una extensión del SGD que añade un término de \"momentum\" a las actualizaciones de los parámetros. Este término ayuda a suavizar las actualizaciones y permite que el optimizador continúe moviéndose en la dirección correcta, incluso si el gradiente cambia de dirección o varía en magnitud. Momentum es particularmente útil para entrenar redes neuronales profundas.\n",
    "\n",
    "### RMSprop\n",
    "RMSprop es un algoritmo de optimización que utiliza un promedio móvil del gradiente al cuadrado para adaptar la tasa de aprendizaje de cada parámetro. Esto ayuda a evitar oscilaciones en las actualizaciones de los parámetros y puede mejorar la convergencia en algunos casos.\n",
    "\n",
    "### Adam\n",
    "Adam es un algoritmo de optimización popular que combina las ideas de momentum y RMSprop. Utiliza un promedio móvil tanto del gradiente como de su valor al cuadrado para adaptar la tasa de aprendizaje de cada parámetro. Adam se utiliza a menudo como optimizador predeterminado para modelos de aprendizaje profundo.\n",
    "\n",
    "### AdamW\n",
    "AdamW es una modificación del optimizador Adam que añade decaimiento de peso a las actualizaciones de los parámetros. Esto ayuda a regularizar el modelo y puede mejorar el rendimiento de generalización. Usaremos el optimizador AdamW ya que se adapta mejor a las propiedades del modelo que entrenaremos en este video.\n",
    "\n",
    "Puedes encontrar más optimizadores y detalles en `torch.optim`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, train loss: 2.358, val loss: 2.392\n",
      "step: 1000, train loss: 2.354, val loss: 2.384\n",
      "step: 2000, train loss: 2.350, val loss: 2.374\n",
      "step: 3000, train loss: 2.350, val loss: 2.369\n",
      "step: 4000, train loss: 2.355, val loss: 2.376\n",
      "step: 5000, train loss: 2.343, val loss: 2.378\n",
      "step: 6000, train loss: 2.361, val loss: 2.370\n",
      "step: 7000, train loss: 2.352, val loss: 2.375\n",
      "step: 8000, train loss: 2.350, val loss: 2.375\n",
      "step: 9000, train loss: 2.356, val loss: 2.361\n",
      "step: 10000, train loss: 2.344, val loss: 2.370\n",
      "step: 11000, train loss: 2.358, val loss: 2.373\n",
      "step: 12000, train loss: 2.332, val loss: 2.368\n",
      "step: 13000, train loss: 2.342, val loss: 2.366\n",
      "step: 14000, train loss: 2.346, val loss: 2.369\n",
      "step: 15000, train loss: 2.351, val loss: 2.373\n",
      "step: 16000, train loss: 2.348, val loss: 2.388\n",
      "step: 17000, train loss: 2.341, val loss: 2.357\n",
      "step: 18000, train loss: 2.347, val loss: 2.374\n",
      "step: 19000, train loss: 2.341, val loss: 2.363\n",
      "step: 20000, train loss: 2.345, val loss: 2.365\n",
      "step: 21000, train loss: 2.341, val loss: 2.365\n",
      "step: 22000, train loss: 2.355, val loss: 2.371\n",
      "step: 23000, train loss: 2.346, val loss: 2.363\n",
      "step: 24000, train loss: 2.338, val loss: 2.375\n",
      "step: 25000, train loss: 2.334, val loss: 2.364\n",
      "step: 26000, train loss: 2.351, val loss: 2.363\n",
      "step: 27000, train loss: 2.341, val loss: 2.372\n",
      "step: 28000, train loss: 2.341, val loss: 2.368\n",
      "step: 29000, train loss: 2.333, val loss: 2.378\n",
      "step: 30000, train loss: 2.338, val loss: 2.359\n",
      "step: 31000, train loss: 2.347, val loss: 2.364\n",
      "step: 32000, train loss: 2.325, val loss: 2.362\n",
      "step: 33000, train loss: 2.330, val loss: 2.377\n",
      "step: 34000, train loss: 2.347, val loss: 2.361\n",
      "step: 35000, train loss: 2.341, val loss: 2.366\n",
      "step: 36000, train loss: 2.331, val loss: 2.357\n",
      "step: 37000, train loss: 2.342, val loss: 2.367\n",
      "step: 38000, train loss: 2.349, val loss: 2.364\n",
      "step: 39000, train loss: 2.333, val loss: 2.363\n",
      "step: 40000, train loss: 2.338, val loss: 2.358\n",
      "step: 41000, train loss: 2.346, val loss: 2.361\n",
      "step: 42000, train loss: 2.333, val loss: 2.376\n",
      "step: 43000, train loss: 2.344, val loss: 2.356\n",
      "step: 44000, train loss: 2.330, val loss: 2.368\n",
      "step: 45000, train loss: 2.347, val loss: 2.365\n",
      "step: 46000, train loss: 2.324, val loss: 2.360\n",
      "step: 47000, train loss: 2.349, val loss: 2.374\n",
      "step: 48000, train loss: 2.329, val loss: 2.356\n",
      "step: 49000, train loss: 2.336, val loss: 2.375\n",
      "step: 50000, train loss: 2.331, val loss: 2.355\n",
      "step: 51000, train loss: 2.322, val loss: 2.350\n",
      "step: 52000, train loss: 2.335, val loss: 2.364\n",
      "step: 53000, train loss: 2.342, val loss: 2.371\n",
      "step: 54000, train loss: 2.341, val loss: 2.359\n",
      "step: 55000, train loss: 2.338, val loss: 2.357\n",
      "step: 56000, train loss: 2.335, val loss: 2.368\n",
      "step: 57000, train loss: 2.335, val loss: 2.360\n",
      "step: 58000, train loss: 2.332, val loss: 2.361\n",
      "step: 59000, train loss: 2.335, val loss: 2.358\n",
      "step: 60000, train loss: 2.326, val loss: 2.359\n",
      "step: 61000, train loss: 2.340, val loss: 2.345\n",
      "step: 62000, train loss: 2.335, val loss: 2.359\n",
      "step: 63000, train loss: 2.331, val loss: 2.361\n",
      "step: 64000, train loss: 2.344, val loss: 2.352\n",
      "step: 65000, train loss: 2.346, val loss: 2.361\n",
      "step: 66000, train loss: 2.340, val loss: 2.347\n",
      "step: 67000, train loss: 2.328, val loss: 2.358\n",
      "step: 68000, train loss: 2.350, val loss: 2.358\n",
      "step: 69000, train loss: 2.327, val loss: 2.361\n",
      "step: 70000, train loss: 2.339, val loss: 2.354\n",
      "step: 71000, train loss: 2.334, val loss: 2.365\n",
      "step: 72000, train loss: 2.339, val loss: 2.351\n",
      "step: 73000, train loss: 2.334, val loss: 2.375\n",
      "step: 74000, train loss: 2.339, val loss: 2.355\n",
      "step: 75000, train loss: 2.323, val loss: 2.365\n",
      "step: 76000, train loss: 2.340, val loss: 2.348\n",
      "step: 77000, train loss: 2.334, val loss: 2.352\n",
      "step: 78000, train loss: 2.340, val loss: 2.366\n",
      "step: 79000, train loss: 2.351, val loss: 2.358\n",
      "step: 80000, train loss: 2.340, val loss: 2.358\n",
      "step: 81000, train loss: 2.329, val loss: 2.354\n",
      "step: 82000, train loss: 2.338, val loss: 2.359\n",
      "step: 83000, train loss: 2.340, val loss: 2.365\n",
      "step: 84000, train loss: 2.340, val loss: 2.355\n",
      "step: 85000, train loss: 2.335, val loss: 2.366\n",
      "step: 86000, train loss: 2.333, val loss: 2.364\n",
      "step: 87000, train loss: 2.332, val loss: 2.359\n",
      "step: 88000, train loss: 2.324, val loss: 2.365\n",
      "step: 89000, train loss: 2.339, val loss: 2.353\n",
      "step: 90000, train loss: 2.327, val loss: 2.353\n",
      "step: 91000, train loss: 2.337, val loss: 2.367\n",
      "step: 92000, train loss: 2.337, val loss: 2.351\n",
      "step: 93000, train loss: 2.328, val loss: 2.367\n",
      "step: 94000, train loss: 2.321, val loss: 2.356\n",
      "step: 95000, train loss: 2.340, val loss: 2.359\n",
      "step: 96000, train loss: 2.336, val loss: 2.354\n",
      "step: 97000, train loss: 2.343, val loss: 2.364\n",
      "step: 98000, train loss: 2.337, val loss: 2.368\n",
      "step: 99000, train loss: 2.356, val loss: 2.352\n",
      "2.2750473022460938\n"
     ]
    }
   ],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    if iter % eval_iters == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step: {iter}, train loss: {losses['train']:.3f}, val loss: {losses['val']:.3f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model.forward(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ct, gon w thare o ain Doure\n",
      "n t of watrerily ain, “I\n",
      "g my:\n",
      "\n",
      "he\n",
      "\n",
      "\n",
      "\n",
      "Bu y ad, smawo bevomat san s, asuisowan’t is ty.\n",
      "\n",
      "herrsitheplind foumacan “I ore sk I n heapidinalaleroonecaigrinee ache y m.”\n",
      "fa angay and Oftherey reriss.”\n",
      "\n",
      "“Wonothed apprendou, can trofersiche ve ll,” e todo the the w. anngros hed t me r be Emy red y othe:\n",
      "was rasth,\n",
      "Aninel whet, d t ingherind, omly\n",
      "“Ohe st’ms mys. t He o seerearcerickid teshend w. ed Ifon t ond s vecisare wioee Cis ge “s That ance heyond thef Winled,”\n",
      "\n",
      "Ththe n\n"
     ]
    }
   ],
   "source": [
    "# context es el tensor que contiene el índice de inicio para la generación de texto.\n",
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "\n",
    "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
